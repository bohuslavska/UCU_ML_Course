{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f5847e-ef83-4693-983b-fe45dff6f9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain-community\n",
    "# ! pip install sentence-transformers\n",
    "# ! pip install faiss-cpu\n",
    "# ! pip install -U langchain-core langchain-mistralai\n",
    "# ! pip install mistralai\n",
    "# ! pip install omegaconf\n",
    "# ! pip install torch==2.1.0\n",
    "# ! pip install gradio\n",
    "# ! pip install ragas\n",
    "# ! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa6fa0ce-c09e-4fa5-84a0-ea37e1bf0176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datasets import Dataset \n",
    "import gradio as gr\n",
    "import openai\n",
    "import omegaconf\n",
    "import torch\n",
    "from ragas.metrics import (context_precision,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_relevancy,\n",
    "    context_entity_recall,\n",
    "    answer_correctness\n",
    ")\n",
    "from ragas import evaluate\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from code_base.inference import PropInferenceWrapper\n",
    "from keys import MISTRAL_KEY, OPENAI_KEY\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea538b8-0d85-47f2-9c10-16a950c0e781",
   "metadata": {},
   "source": [
    "# Creating Dataset with Techniques and their Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c765d2-3dc8-4e64-981c-f91551e9e8ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's create dataset with the labels and their explanations.\n",
    "\n",
    "indices = ['Slogans', 'Black-and-White_Fallacy', 'Loaded_Language', 'Flag-Waving',\n",
    " 'Name_Calling,Labeling','Whataboutism', 'Causal_Oversimplification', 'Exaggeration,Minimisation',\n",
    " 'Doubt', 'Appeal_to_Authority', 'Repetition', 'Appeal_to_fear-prejudice', 'Red_Herring',\n",
    " 'Thought-terminating_Cliches', 'Bandwagon', 'Reductio_ad_hitlerum', 'Obfuscation',\n",
    " 'Intentional_Vagueness,Confusion', 'Straw_Men']\n",
    "\n",
    "labels_dataframe = pd.DataFrame(columns = ['ground_truth'], index = indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a682d9c-33ca-47c4-b6ca-cbac7f0c1098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_dataframe.loc['Slogans', 'ground_truth'] = \"A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals.\"\n",
    "labels_dataframe.loc['Black-and-White_Fallacy', 'ground_truth'] = \"Presenting two alternative options as the only possibilities, when in fact more possibilities exist. As an the extreme case, tell the audience exactly what actions to take, eliminating any other possible choices (Dictatorship).\"\n",
    "labels_dataframe.loc['Loaded_Language', 'ground_truth'] = \"Using words/phrases with strong emotional implications (positive or negative) to influence an audience\"\n",
    "labels_dataframe.loc['Flag-Waving', 'ground_truth'] = \"Playing on strong national feeling (or to any group; e.g., race, gender, political preference) to justify or promote an action or idea\"\n",
    "labels_dataframe.loc['Name_Calling,Labeling', 'ground_truth'] = \"Labeling the object of the propaganda campaign as either something the target audience fears, hates, finds undesirable or loves, praises.\"\n",
    "labels_dataframe.loc['Whataboutism', 'ground_truth'] = \"A technique that attempts to discredit an opponent's position by charging them with hypocrisy without directly disproving their argument.\"\n",
    "labels_dataframe.loc['Causal_Oversimplification', 'ground_truth'] = \"Assuming a single cause or reason when there are actually multiple causes for an issue. It includes transferring blame to one person or group of people without investigating the complexities of the issue\"\n",
    "labels_dataframe.loc['Exaggeration,Minimisation', 'ground_truth'] = \"Either representing something in an excessive manner: making things larger, better, worse (e.g., 'the best of the best', 'quality guaranteed') or making something seem less important or smaller than it really is (e.g., saying that an insult was just a joke).\"\n",
    "labels_dataframe.loc['Doubt', 'ground_truth'] = \"Questioning the credibility of someone or something.\"\n",
    "labels_dataframe.loc['Appeal_to_Authority', 'ground_truth'] = \"Stating that a claim is true simply because a valid authority or expert on the issue said it was true, without any other supporting evidence offered. We consider the special case in which the reference is not an authority or an expert in this technique, altough it is referred to as Testimonial in literature.\"\n",
    "labels_dataframe.loc['Repetition', 'ground_truth'] = \"Repeating the same message over and over again so that the audience will eventually accept it.\"\n",
    "labels_dataframe.loc['Appeal_to_fear-prejudice', 'ground_truth'] = \"Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative. In some cases the support is built based on preconceived judgements.\"\n",
    "labels_dataframe.loc['Red_Herring', 'ground_truth'] = \"Introducing irrelevant material to the issue being discussed, so that everyone's attention is diverted away from the points made.\"\n",
    "labels_dataframe.loc['Thought-terminating_Cliches', 'ground_truth'] = \"Words or phrases that discourage critical thought and meaningful discussion about a given topic. They are typically short, generic sentences that offer seemingly simple answers to complex questions or that distract attention away from other lines of thought.\"\n",
    "labels_dataframe.loc['Bandwagon', 'ground_truth'] = \"Attempting to persuade the target audience to join in and take the course of action because 'everyone else is taking the same action'.\"\n",
    "labels_dataframe.loc['Reductio_ad_hitlerum', 'ground_truth'] = \"Persuading an audience to disapprove an action or idea by suggesting that the idea is popular with groups hated in contempt by the target audience. It can refer to any person or concept with a negative connotation.\"\n",
    "labels_dataframe.loc['Obfuscation,Intentional_Vagueness,Confusion', 'ground_truth'] = \"Using words which are deliberately not clear so that the audience may have its own interpretations.For example when an unclear phrase with multiple definitions is used within the argument and, therefore, it does not support the conclusion.\"\n",
    "labels_dataframe.loc['Straw_Men', 'ground_truth'] = \"When an opponent's proposition is substituted with a similar one which is then refuted in place of the original proposition.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6066e620-81b7-48d1-abba-932e0cfc6e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "\n",
    "labels_dataframe.loc['Slogans', 'ground_truth']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b5785-c5ce-4b7f-a0fc-43d025e1d8a7",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f846d63-4dd1-422f-8f39-81f1d87227e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "      <th>location_markers</th>\n",
       "      <th>extracted_manupulation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Swedish PM does not rule out use of army to en...</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>[1243, 1254]</td>\n",
       "      <td>stamped out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swedish PM does not rule out use of army to en...</td>\n",
       "      <td>Flag-Waving</td>\n",
       "      <td>[1874, 2026]</td>\n",
       "      <td>This is the new Sweden; the new, exciting dyna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Israel Takes On the Shia Crescent\\n\\nDespite I...</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>[86, 106]</td>\n",
       "      <td>reckless appeasement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Israel Takes On the Shia Crescent\\n\\nDespite I...</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>[820, 830]</td>\n",
       "      <td>disastrous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Israel Takes On the Shia Crescent\\n\\nDespite I...</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>[2377, 2400]</td>\n",
       "      <td>\\n“It’s a new monster.”\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>SNL Indian Comedian Silenced for \"Offensive Jo...</td>\n",
       "      <td>Slogans</td>\n",
       "      <td>[191, 213]</td>\n",
       "      <td>the Left killed comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>SNL Indian Comedian Silenced for \"Offensive Jo...</td>\n",
       "      <td>Exaggeration,Minimisation</td>\n",
       "      <td>[1043, 1148]</td>\n",
       "      <td>no one looks in the mirror and thinks, ‘this b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>SNL Indian Comedian Silenced for \"Offensive Jo...</td>\n",
       "      <td>Name_Calling,Labeling</td>\n",
       "      <td>[1164, 1184]</td>\n",
       "      <td>Columbia snowflakes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>SNL Indian Comedian Silenced for \"Offensive Jo...</td>\n",
       "      <td>Exaggeration,Minimisation</td>\n",
       "      <td>[1607, 1674]</td>\n",
       "      <td>Comrades, these jokes you have been listening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>SNL Indian Comedian Silenced for \"Offensive Jo...</td>\n",
       "      <td>Exaggeration,Minimisation</td>\n",
       "      <td>[2611, 2651]</td>\n",
       "      <td>I'm sure Patel felt very, like, accepted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>927 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  news  \\\n",
       "0    Swedish PM does not rule out use of army to en...   \n",
       "1    Swedish PM does not rule out use of army to en...   \n",
       "2    Israel Takes On the Shia Crescent\\n\\nDespite I...   \n",
       "3    Israel Takes On the Shia Crescent\\n\\nDespite I...   \n",
       "4    Israel Takes On the Shia Crescent\\n\\nDespite I...   \n",
       "..                                                 ...   \n",
       "922  SNL Indian Comedian Silenced for \"Offensive Jo...   \n",
       "923  SNL Indian Comedian Silenced for \"Offensive Jo...   \n",
       "924  SNL Indian Comedian Silenced for \"Offensive Jo...   \n",
       "925  SNL Indian Comedian Silenced for \"Offensive Jo...   \n",
       "926  SNL Indian Comedian Silenced for \"Offensive Jo...   \n",
       "\n",
       "                         label location_markers  \\\n",
       "0              Loaded_Language     [1243, 1254]   \n",
       "1                  Flag-Waving     [1874, 2026]   \n",
       "2              Loaded_Language        [86, 106]   \n",
       "3              Loaded_Language       [820, 830]   \n",
       "4              Loaded_Language     [2377, 2400]   \n",
       "..                         ...              ...   \n",
       "922                    Slogans       [191, 213]   \n",
       "923  Exaggeration,Minimisation     [1043, 1148]   \n",
       "924      Name_Calling,Labeling     [1164, 1184]   \n",
       "925  Exaggeration,Minimisation     [1607, 1674]   \n",
       "926  Exaggeration,Minimisation     [2611, 2651]   \n",
       "\n",
       "                                extracted_manupulation  \n",
       "0                                          stamped out  \n",
       "1    This is the new Sweden; the new, exciting dyna...  \n",
       "2                                 reckless appeasement  \n",
       "3                                           disastrous  \n",
       "4                            \\n“It’s a new monster.”\\n  \n",
       "..                                                 ...  \n",
       "922                             the Left killed comedy  \n",
       "923  no one looks in the mirror and thinks, ‘this b...  \n",
       "924                               Columbia snowflakes   \n",
       "925  Comrades, these jokes you have been listening ...  \n",
       "926           I'm sure Patel felt very, like, accepted  \n",
       "\n",
       "[927 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download test dataset.\n",
    "\n",
    "test_data = pd.read_csv('test_dataset.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f66354f-1b38-48b1-a051-8fee338a3f77",
   "metadata": {},
   "source": [
    "# Creating Dataset for RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc8ab2fa-94cc-4a18-99db-a41adeb36dea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config ...\n",
      "Config loaded\n",
      "Loading model ...\n",
      "Treshold value = 0.1. Recommended treshold = 0.5\n",
      "Model loaded\n",
      "Loading tokenizer ...\n",
      "Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "#Create a Mistral client, load the transformer, and specify some parameters\n",
    "\n",
    "MISTRAL_API_KEY = MISTRAL_KEY\n",
    "client = MistralClient(api_key=MISTRAL_API_KEY)\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "t_inf = PropInferenceWrapper(\n",
    "config_path=\"config.yaml\",\n",
    "chkp_path=\"best.pth\",\n",
    "tresh=0.1)\n",
    "\n",
    "prompt_template = \"\"\"Human: You are a brilliant media expert skilled at explaining manipulation techniques in news articles. \n",
    "Your colleagues have identified several such manipulations but did not provide explanations for their classifications. \n",
    "We trust their judgment as correct. Your task is to logically explain why each one fits its assigned label using using provided context. If you're unsure, simply state that you don't know — avoid making up an answer. \n",
    "Do not doubt labelling of your colleagues. \n",
    "Instructions:\n",
    "Please limit your explanation to up to 20 words for each example.\n",
    "Never repeat query in your answer!\n",
    "Format your output as bullet points where each line should look like this: \n",
    "label - detected example - explanation (up to 15 words). \n",
    "Always (!) begin line with label not with detected example!!! For example:\n",
    "- Exaggeration, Minimisation - \"done next to nothing\" - makes something seem less important or smaller than it really is\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f2cdee5-1846-4fb2-8669-2c6b1e398e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_dataset(news_article):\n",
    "    \"\"\"\n",
    "    Prepares the data needed for evaluating manipulative phrases in a news article by processing \n",
    "    the text to detect significant phrases, querying a question-answering system for explanations, \n",
    "    and organizing the resulting data into lists for further analysis.\n",
    "\n",
    "    Parameters:\n",
    "    news_article (str): A string containing the news article text to be analyzed.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Predict tags for each word in the article\n",
    "    result = t_inf.predict(news_article)\n",
    "    words, tags = result[0], result[1]\n",
    "    \n",
    "    # Create DataFrame from words and tags, filter out tags 'O'\n",
    "    df = pd.DataFrame(list(zip(words, tags)), columns=['Word', 'Tag'])\n",
    "    filtered_df = df[df['Tag'] != 'O']\n",
    "    filtered_df['Word'] = filtered_df['Word'].str.strip()\n",
    "    \n",
    "    # Group words by their tags, create a dictionary of tags and their corresponding phrases\n",
    "    filtered_df['group'] = (filtered_df['Tag'] != filtered_df['Tag'].shift(1)).cumsum()\n",
    "    grouped = filtered_df.groupby(['Tag', 'group'])['Word'].apply(' '.join).reset_index()\n",
    "    manipulation_dict = grouped.groupby('Tag')['Word'].apply(list).to_dict()\n",
    "\n",
    "    llm = ChatMistralAI(api_key=MISTRAL_API_KEY, model=model, temperature=0.1)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=modelPath, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "    \n",
    "    # Load a FAISS vector store for efficient similarity search\n",
    "    vectorstore_faiss = FAISS.load_local(\"faiss_db_v3\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    # Template for generating prompts for the QA model\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    # Setup a retrieval-based QA system\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore_faiss.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    \n",
    "    # Generate questions and get answers from the QA system\n",
    "    for k, v in manipulation_dict.items():\n",
    "        question = f\"Why do the detected manipulations from this list {v} belong to the label {k}?\"\n",
    "        answer = qa({\"query\": question})\n",
    "        list_questions.append(question)\n",
    "        \n",
    "        # Clean the answer text\n",
    "        answer['result'] = answer['result'].strip()\n",
    "        answer['result'] = re.sub(r'^[ \\t]+', '', answer['result'], flags=re.MULTILINE)\n",
    "        list_answers.append(answer['result'])\n",
    "        \n",
    "        # Extract and clean page content from source documents\n",
    "        content_start = str(answer['source_documents']).find(\"page_content='\") + len(\"page_content='\")\n",
    "        content_end = str(answer['source_documents']).find(\"metadata\", content_start)\n",
    "        page_content = str(answer['source_documents'])[content_start:content_end]\n",
    "        page_content = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', page_content)\n",
    "        list_context.append([page_content])\n",
    "        \n",
    "        # Retrieve ground truth for the tag from a predefined DataFrame\n",
    "        list_ground.append(labels_dataframe.loc[k, 'ground_truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb02bc02-8936-4e9b-9cf0-c73d63d2d23b",
   "metadata": {},
   "source": [
    "# Creating Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cccb2c9-a9de-4d36-b262-399d07b79c59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 unique articles in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "#Let's find out how many unique articles contains test dataset.\n",
    "\n",
    "all_articles = list(test_data['news'].unique())\n",
    "print(f'There are {len(all_articles)} unique articles in the test dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85d93a0e-51dc-46b8-9c1a-6a8669261a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.73s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.69s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing is used to handle long samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#Create lists for the future evaluation dataset.\n",
    "list_questions = []\n",
    "list_answers = []\n",
    "list_context = []\n",
    "list_ground = []\n",
    "\n",
    "for article in all_articles:\n",
    "    eval_dataset(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc90ff6a-6dd9-4598-bbe3-6eefb8648e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset with the shape (313, 4) is saved.\n"
     ]
    }
   ],
   "source": [
    "#Save evaluation dataset.\n",
    "\n",
    "eval_dict = {'question': list_questions, 'answer': list_answers, 'contexts': list_context, 'ground_truth': list_ground}\n",
    "eval_data = pd.DataFrame(eval_dict)\n",
    "eval_data.to_csv('Eval_dataset_v3.csv')\n",
    "print(f'Evaluation dataset with the shape {eval_data.shape} is saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4672934-67d5-4fd4-811d-6fef7912e6e6",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbde5b13-a91e-47b3-a772-b3313c6e7b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Download saved dataset and prepare it for Ragas evaluation.\n",
    "\n",
    "dataset_eval = pd.read_csv('Eval_dataset_v3.csv')\n",
    "dataset_eval['contexts'] = dataset_eval['contexts'].apply(lambda x: [x])\n",
    "dataset_eval = Dataset.from_pandas(dataset_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3716cdd7-98e3-4883-b781-d245a2408b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0720f30d03940dfa59bd03fe17aa4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf26b04cb3240809d223162e1482d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea466477b7a49f2a6a62f6251fc7847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e900d802778424581db035dacddc8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4893ec6897c1440ca8ac6faff2950468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2c14b8af0d44919fd34be8616bbfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "#Evaluate 250 samples because of OPENAI tokens limitations.\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY\n",
    "gpt_turbo = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "metrics = [faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "        context_relevancy,\n",
    "        answer_correctness]\n",
    "\n",
    "scores = []\n",
    "for m in metrics:\n",
    "    score = evaluate(dataset_eval.select(range(250)), metrics=[m], llm = gpt_turbo)\n",
    "    scores.append(score)\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e430b4-e1f5-4b01-8ead-a5bd8b4404f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.363135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_relevancy</th>\n",
       "      <td>0.711509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_recall</th>\n",
       "      <td>0.786108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_precision</th>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_relevancy</th>\n",
       "      <td>0.564538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_correctness</th>\n",
       "      <td>0.730049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Scores\n",
       "Index                       \n",
       "faithfulness        0.363135\n",
       "answer_relevancy    0.711509\n",
       "context_recall      0.786108\n",
       "context_precision   0.676000\n",
       "context_relevancy   0.564538\n",
       "answer_correctness  0.730049"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make pandas dataset for presentation purposes.\n",
    "\n",
    "data_transformed = {list(d.keys())[0]: list(d.values())[0] for d in scores}\n",
    "scores_df = pd.DataFrame(list(data_transformed.items()), columns=['Index', 'Scores'])\n",
    "scores_df.set_index('Index', inplace=True)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3eada5-e2ba-4d40-bae0-58810868feb4",
   "metadata": {},
   "source": [
    "The retrieval metrics are generally high. However, the faithfulness score, which measures the factual consistency of the generated answer against the given context, is low. I should work on refining the prompt to better integrate the context.\n",
    "\n",
    "Additionally, the context relevancy score is low. This metric assesses the relevance of the retrieved context based on both the question and the contexts. To improve this, I might consider not only retrieving the four closest documents but also establishing a threshold. For instance, setting up the retriever with a similarity score threshold might be effective (retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.7})).\""
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
